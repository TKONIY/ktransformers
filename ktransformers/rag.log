Unrecognized keys in `rope_scaling` for 'rope_type'='dynamic': {'type'}
using custom modeling_xxx.py.
using default_optimize_rule for LlamaForCausalLM
Injecting model as ktransformers.operators.models . KLlamaModel
local_windows_len: 4096, topk: 96, dense_layer_num: 0, kv_type: FP16, anchor_type: DYNAMIC, preselect_block: False, preselect_block_count: 96, token_step: 1, layer_step: 1
Injecting model.embed_tokens as default
Injecting model.layers as default
Injecting model.layers.0 as default
Injecting model.layers.0.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.0.self_attn.q_proj as default
Injecting model.layers.0.self_attn.k_proj as default
Injecting model.layers.0.self_attn.v_proj as default
Injecting model.layers.0.self_attn.o_proj as default
Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.0.mlp as default
Injecting model.layers.0.mlp.gate_proj as default
Injecting model.layers.0.mlp.up_proj as default
Injecting model.layers.0.mlp.down_proj as default
Injecting model.layers.0.mlp.act_fn as default
Injecting model.layers.0.input_layernorm as default
Injecting model.layers.0.post_attention_layernorm as default
Injecting model.layers.1 as default
Injecting model.layers.1.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.1.self_attn.q_proj as default
Injecting model.layers.1.self_attn.k_proj as default
Injecting model.layers.1.self_attn.v_proj as default
Injecting model.layers.1.self_attn.o_proj as default
Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.1.mlp as default
Injecting model.layers.1.mlp.gate_proj as default
Injecting model.layers.1.mlp.up_proj as default
Injecting model.layers.1.mlp.down_proj as default
Injecting model.layers.1.mlp.act_fn as default
Injecting model.layers.1.input_layernorm as default
Injecting model.layers.1.post_attention_layernorm as default
Injecting model.layers.2 as default
Injecting model.layers.2.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.2.self_attn.q_proj as default
Injecting model.layers.2.self_attn.k_proj as default
Injecting model.layers.2.self_attn.v_proj as default
Injecting model.layers.2.self_attn.o_proj as default
Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.2.mlp as default
Injecting model.layers.2.mlp.gate_proj as default
Injecting model.layers.2.mlp.up_proj as default
Injecting model.layers.2.mlp.down_proj as default
Injecting model.layers.2.mlp.act_fn as default
Injecting model.layers.2.input_layernorm as default
Injecting model.layers.2.post_attention_layernorm as default
Injecting model.layers.3 as default
Injecting model.layers.3.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.3.self_attn.q_proj as default
Injecting model.layers.3.self_attn.k_proj as default
Injecting model.layers.3.self_attn.v_proj as default
Injecting model.layers.3.self_attn.o_proj as default
Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.3.mlp as default
Injecting model.layers.3.mlp.gate_proj as default
Injecting model.layers.3.mlp.up_proj as default
Injecting model.layers.3.mlp.down_proj as default
Injecting model.layers.3.mlp.act_fn as default
Injecting model.layers.3.input_layernorm as default
Injecting model.layers.3.post_attention_layernorm as default
Injecting model.layers.4 as default
Injecting model.layers.4.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.4.self_attn.q_proj as default
Injecting model.layers.4.self_attn.k_proj as default
Injecting model.layers.4.self_attn.v_proj as default
Injecting model.layers.4.self_attn.o_proj as default
Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.4.mlp as default
Injecting model.layers.4.mlp.gate_proj as default
Injecting model.layers.4.mlp.up_proj as default
Injecting model.layers.4.mlp.down_proj as default
Injecting model.layers.4.mlp.act_fn as default
Injecting model.layers.4.input_layernorm as default
Injecting model.layers.4.post_attention_layernorm as default
Injecting model.layers.5 as default
Injecting model.layers.5.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.5.self_attn.q_proj as default
Injecting model.layers.5.self_attn.k_proj as default
Injecting model.layers.5.self_attn.v_proj as default
Injecting model.layers.5.self_attn.o_proj as default
Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.5.mlp as default
Injecting model.layers.5.mlp.gate_proj as default
Injecting model.layers.5.mlp.up_proj as default
Injecting model.layers.5.mlp.down_proj as default
Injecting model.layers.5.mlp.act_fn as default
Injecting model.layers.5.input_layernorm as default
Injecting model.layers.5.post_attention_layernorm as default
Injecting model.layers.6 as default
Injecting model.layers.6.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.6.self_attn.q_proj as default
Injecting model.layers.6.self_attn.k_proj as default
Injecting model.layers.6.self_attn.v_proj as default
Injecting model.layers.6.self_attn.o_proj as default
Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.6.mlp as default
Injecting model.layers.6.mlp.gate_proj as default
Injecting model.layers.6.mlp.up_proj as default
Injecting model.layers.6.mlp.down_proj as default
Injecting model.layers.6.mlp.act_fn as default
Injecting model.layers.6.input_layernorm as default
Injecting model.layers.6.post_attention_layernorm as default
Injecting model.layers.7 as default
Injecting model.layers.7.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.7.self_attn.q_proj as default
Injecting model.layers.7.self_attn.k_proj as default
Injecting model.layers.7.self_attn.v_proj as default
Injecting model.layers.7.self_attn.o_proj as default
Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.7.mlp as default
Injecting model.layers.7.mlp.gate_proj as default
Injecting model.layers.7.mlp.up_proj as default
Injecting model.layers.7.mlp.down_proj as default
Injecting model.layers.7.mlp.act_fn as default
Injecting model.layers.7.input_layernorm as default
Injecting model.layers.7.post_attention_layernorm as default
Injecting model.layers.8 as default
Injecting model.layers.8.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.8.self_attn.q_proj as default
Injecting model.layers.8.self_attn.k_proj as default
Injecting model.layers.8.self_attn.v_proj as default
Injecting model.layers.8.self_attn.o_proj as default
Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.8.mlp as default
Injecting model.layers.8.mlp.gate_proj as default
Injecting model.layers.8.mlp.up_proj as default
Injecting model.layers.8.mlp.down_proj as default
Injecting model.layers.8.mlp.act_fn as default
Injecting model.layers.8.input_layernorm as default
Injecting model.layers.8.post_attention_layernorm as default
Injecting model.layers.9 as default
Injecting model.layers.9.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.9.self_attn.q_proj as default
Injecting model.layers.9.self_attn.k_proj as default
Injecting model.layers.9.self_attn.v_proj as default
Injecting model.layers.9.self_attn.o_proj as default
Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.9.mlp as default
Injecting model.layers.9.mlp.gate_proj as default
Injecting model.layers.9.mlp.up_proj as default
Injecting model.layers.9.mlp.down_proj as default
Injecting model.layers.9.mlp.act_fn as default
Injecting model.layers.9.input_layernorm as default
Injecting model.layers.9.post_attention_layernorm as default
Injecting model.layers.10 as default
Injecting model.layers.10.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.10.self_attn.q_proj as default
Injecting model.layers.10.self_attn.k_proj as default
Injecting model.layers.10.self_attn.v_proj as default
Injecting model.layers.10.self_attn.o_proj as default
Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.10.mlp as default
Injecting model.layers.10.mlp.gate_proj as default
Injecting model.layers.10.mlp.up_proj as default
Injecting model.layers.10.mlp.down_proj as default
Injecting model.layers.10.mlp.act_fn as default
Injecting model.layers.10.input_layernorm as default
Injecting model.layers.10.post_attention_layernorm as default
Injecting model.layers.11 as default
Injecting model.layers.11.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.11.self_attn.q_proj as default
Injecting model.layers.11.self_attn.k_proj as default
Injecting model.layers.11.self_attn.v_proj as default
Injecting model.layers.11.self_attn.o_proj as default
Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.11.mlp as default
Injecting model.layers.11.mlp.gate_proj as default
Injecting model.layers.11.mlp.up_proj as default
Injecting model.layers.11.mlp.down_proj as default
Injecting model.layers.11.mlp.act_fn as default
Injecting model.layers.11.input_layernorm as default
Injecting model.layers.11.post_attention_layernorm as default
Injecting model.layers.12 as default
Injecting model.layers.12.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.12.self_attn.q_proj as default
Injecting model.layers.12.self_attn.k_proj as default
Injecting model.layers.12.self_attn.v_proj as default
Injecting model.layers.12.self_attn.o_proj as default
Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.12.mlp as default
Injecting model.layers.12.mlp.gate_proj as default
Injecting model.layers.12.mlp.up_proj as default
Injecting model.layers.12.mlp.down_proj as default
Injecting model.layers.12.mlp.act_fn as default
Injecting model.layers.12.input_layernorm as default
Injecting model.layers.12.post_attention_layernorm as default
Injecting model.layers.13 as default
Injecting model.layers.13.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.13.self_attn.q_proj as default
Injecting model.layers.13.self_attn.k_proj as default
Injecting model.layers.13.self_attn.v_proj as default
Injecting model.layers.13.self_attn.o_proj as default
Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.13.mlp as default
Injecting model.layers.13.mlp.gate_proj as default
Injecting model.layers.13.mlp.up_proj as default
Injecting model.layers.13.mlp.down_proj as default
Injecting model.layers.13.mlp.act_fn as default
Injecting model.layers.13.input_layernorm as default
Injecting model.layers.13.post_attention_layernorm as default
Injecting model.layers.14 as default
Injecting model.layers.14.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.14.self_attn.q_proj as default
Injecting model.layers.14.self_attn.k_proj as default
Injecting model.layers.14.self_attn.v_proj as default
Injecting model.layers.14.self_attn.o_proj as default
Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.14.mlp as default
Injecting model.layers.14.mlp.gate_proj as default
Injecting model.layers.14.mlp.up_proj as default
Injecting model.layers.14.mlp.down_proj as default
Injecting model.layers.14.mlp.act_fn as default
Injecting model.layers.14.input_layernorm as default
Injecting model.layers.14.post_attention_layernorm as default
Injecting model.layers.15 as default
Injecting model.layers.15.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.15.self_attn.q_proj as default
Injecting model.layers.15.self_attn.k_proj as default
Injecting model.layers.15.self_attn.v_proj as default
Injecting model.layers.15.self_attn.o_proj as default
Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.15.mlp as default
Injecting model.layers.15.mlp.gate_proj as default
Injecting model.layers.15.mlp.up_proj as default
Injecting model.layers.15.mlp.down_proj as default
Injecting model.layers.15.mlp.act_fn as default
Injecting model.layers.15.input_layernorm as default
Injecting model.layers.15.post_attention_layernorm as default
Injecting model.layers.16 as default
Injecting model.layers.16.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.16.self_attn.q_proj as default
Injecting model.layers.16.self_attn.k_proj as default
Injecting model.layers.16.self_attn.v_proj as default
Injecting model.layers.16.self_attn.o_proj as default
Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.16.mlp as default
Injecting model.layers.16.mlp.gate_proj as default
Injecting model.layers.16.mlp.up_proj as default
Injecting model.layers.16.mlp.down_proj as default
Injecting model.layers.16.mlp.act_fn as default
Injecting model.layers.16.input_layernorm as default
Injecting model.layers.16.post_attention_layernorm as default
Injecting model.layers.17 as default
Injecting model.layers.17.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.17.self_attn.q_proj as default
Injecting model.layers.17.self_attn.k_proj as default
Injecting model.layers.17.self_attn.v_proj as default
Injecting model.layers.17.self_attn.o_proj as default
Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.17.mlp as default
Injecting model.layers.17.mlp.gate_proj as default
Injecting model.layers.17.mlp.up_proj as default
Injecting model.layers.17.mlp.down_proj as default
Injecting model.layers.17.mlp.act_fn as default
Injecting model.layers.17.input_layernorm as default
Injecting model.layers.17.post_attention_layernorm as default
Injecting model.layers.18 as default
Injecting model.layers.18.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.18.self_attn.q_proj as default
Injecting model.layers.18.self_attn.k_proj as default
Injecting model.layers.18.self_attn.v_proj as default
Injecting model.layers.18.self_attn.o_proj as default
Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.18.mlp as default
Injecting model.layers.18.mlp.gate_proj as default
Injecting model.layers.18.mlp.up_proj as default
Injecting model.layers.18.mlp.down_proj as default
Injecting model.layers.18.mlp.act_fn as default
Injecting model.layers.18.input_layernorm as default
Injecting model.layers.18.post_attention_layernorm as default
Injecting model.layers.19 as default
Injecting model.layers.19.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.19.self_attn.q_proj as default
Injecting model.layers.19.self_attn.k_proj as default
Injecting model.layers.19.self_attn.v_proj as default
Injecting model.layers.19.self_attn.o_proj as default
Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.19.mlp as default
Injecting model.layers.19.mlp.gate_proj as default
Injecting model.layers.19.mlp.up_proj as default
Injecting model.layers.19.mlp.down_proj as default
Injecting model.layers.19.mlp.act_fn as default
Injecting model.layers.19.input_layernorm as default
Injecting model.layers.19.post_attention_layernorm as default
Injecting model.layers.20 as default
Injecting model.layers.20.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.20.self_attn.q_proj as default
Injecting model.layers.20.self_attn.k_proj as default
Injecting model.layers.20.self_attn.v_proj as default
Injecting model.layers.20.self_attn.o_proj as default
Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.20.mlp as default
Injecting model.layers.20.mlp.gate_proj as default
Injecting model.layers.20.mlp.up_proj as default
Injecting model.layers.20.mlp.down_proj as default
Injecting model.layers.20.mlp.act_fn as default
Injecting model.layers.20.input_layernorm as default
Injecting model.layers.20.post_attention_layernorm as default
Injecting model.layers.21 as default
Injecting model.layers.21.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.21.self_attn.q_proj as default
Injecting model.layers.21.self_attn.k_proj as default
Injecting model.layers.21.self_attn.v_proj as default
Injecting model.layers.21.self_attn.o_proj as default
Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.21.mlp as default
Injecting model.layers.21.mlp.gate_proj as default
Injecting model.layers.21.mlp.up_proj as default
Injecting model.layers.21.mlp.down_proj as default
Injecting model.layers.21.mlp.act_fn as default
Injecting model.layers.21.input_layernorm as default
Injecting model.layers.21.post_attention_layernorm as default
Injecting model.layers.22 as default
Injecting model.layers.22.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.22.self_attn.q_proj as default
Injecting model.layers.22.self_attn.k_proj as default
Injecting model.layers.22.self_attn.v_proj as default
Injecting model.layers.22.self_attn.o_proj as default
Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.22.mlp as default
Injecting model.layers.22.mlp.gate_proj as default
Injecting model.layers.22.mlp.up_proj as default
Injecting model.layers.22.mlp.down_proj as default
Injecting model.layers.22.mlp.act_fn as default
Injecting model.layers.22.input_layernorm as default
Injecting model.layers.22.post_attention_layernorm as default
Injecting model.layers.23 as default
Injecting model.layers.23.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.23.self_attn.q_proj as default
Injecting model.layers.23.self_attn.k_proj as default
Injecting model.layers.23.self_attn.v_proj as default
Injecting model.layers.23.self_attn.o_proj as default
Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.23.mlp as default
Injecting model.layers.23.mlp.gate_proj as default
Injecting model.layers.23.mlp.up_proj as default
Injecting model.layers.23.mlp.down_proj as default
Injecting model.layers.23.mlp.act_fn as default
Injecting model.layers.23.input_layernorm as default
Injecting model.layers.23.post_attention_layernorm as default
Injecting model.layers.24 as default
Injecting model.layers.24.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.24.self_attn.q_proj as default
Injecting model.layers.24.self_attn.k_proj as default
Injecting model.layers.24.self_attn.v_proj as default
Injecting model.layers.24.self_attn.o_proj as default
Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.24.mlp as default
Injecting model.layers.24.mlp.gate_proj as default
Injecting model.layers.24.mlp.up_proj as default
Injecting model.layers.24.mlp.down_proj as default
Injecting model.layers.24.mlp.act_fn as default
Injecting model.layers.24.input_layernorm as default
Injecting model.layers.24.post_attention_layernorm as default
Injecting model.layers.25 as default
Injecting model.layers.25.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.25.self_attn.q_proj as default
Injecting model.layers.25.self_attn.k_proj as default
Injecting model.layers.25.self_attn.v_proj as default
Injecting model.layers.25.self_attn.o_proj as default
Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.25.mlp as default
Injecting model.layers.25.mlp.gate_proj as default
Injecting model.layers.25.mlp.up_proj as default
Injecting model.layers.25.mlp.down_proj as default
Injecting model.layers.25.mlp.act_fn as default
Injecting model.layers.25.input_layernorm as default
Injecting model.layers.25.post_attention_layernorm as default
Injecting model.layers.26 as default
Injecting model.layers.26.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.26.self_attn.q_proj as default
Injecting model.layers.26.self_attn.k_proj as default
Injecting model.layers.26.self_attn.v_proj as default
Injecting model.layers.26.self_attn.o_proj as default
Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.26.mlp as default
Injecting model.layers.26.mlp.gate_proj as default
Injecting model.layers.26.mlp.up_proj as default
Injecting model.layers.26.mlp.down_proj as default
Injecting model.layers.26.mlp.act_fn as default
Injecting model.layers.26.input_layernorm as default
Injecting model.layers.26.post_attention_layernorm as default
Injecting model.layers.27 as default
Injecting model.layers.27.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.27.self_attn.q_proj as default
Injecting model.layers.27.self_attn.k_proj as default
Injecting model.layers.27.self_attn.v_proj as default
Injecting model.layers.27.self_attn.o_proj as default
Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.27.mlp as default
Injecting model.layers.27.mlp.gate_proj as default
Injecting model.layers.27.mlp.up_proj as default
Injecting model.layers.27.mlp.down_proj as default
Injecting model.layers.27.mlp.act_fn as default
Injecting model.layers.27.input_layernorm as default
Injecting model.layers.27.post_attention_layernorm as default
Injecting model.layers.28 as default
Injecting model.layers.28.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.28.self_attn.q_proj as default
Injecting model.layers.28.self_attn.k_proj as default
Injecting model.layers.28.self_attn.v_proj as default
Injecting model.layers.28.self_attn.o_proj as default
Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.28.mlp as default
Injecting model.layers.28.mlp.gate_proj as default
Injecting model.layers.28.mlp.up_proj as default
Injecting model.layers.28.mlp.down_proj as default
Injecting model.layers.28.mlp.act_fn as default
Injecting model.layers.28.input_layernorm as default
Injecting model.layers.28.post_attention_layernorm as default
Injecting model.layers.29 as default
Injecting model.layers.29.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.29.self_attn.q_proj as default
Injecting model.layers.29.self_attn.k_proj as default
Injecting model.layers.29.self_attn.v_proj as default
Injecting model.layers.29.self_attn.o_proj as default
Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.29.mlp as default
Injecting model.layers.29.mlp.gate_proj as default
Injecting model.layers.29.mlp.up_proj as default
Injecting model.layers.29.mlp.down_proj as default
Injecting model.layers.29.mlp.act_fn as default
Injecting model.layers.29.input_layernorm as default
Injecting model.layers.29.post_attention_layernorm as default
Injecting model.layers.30 as default
Injecting model.layers.30.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.30.self_attn.q_proj as default
Injecting model.layers.30.self_attn.k_proj as default
Injecting model.layers.30.self_attn.v_proj as default
/home/ziyang/nvme7n1p1_mount/ktransformers/ktransformers/util/custom_gguf.py:299: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  values = torch.from_numpy(values)
Injecting model.layers.30.self_attn.o_proj as default
Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.30.mlp as default
Injecting model.layers.30.mlp.gate_proj as default
Injecting model.layers.30.mlp.up_proj as default
Injecting model.layers.30.mlp.down_proj as default
Injecting model.layers.30.mlp.act_fn as default
Injecting model.layers.30.input_layernorm as default
Injecting model.layers.30.post_attention_layernorm as default
Injecting model.layers.31 as default
Injecting model.layers.31.self_attn as ktransformers.operators.attention . KLlamaAttention
Injecting model.layers.31.self_attn.q_proj as default
Injecting model.layers.31.self_attn.k_proj as default
Injecting model.layers.31.self_attn.v_proj as default
Injecting model.layers.31.self_attn.o_proj as default
Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting model.layers.31.mlp as default
Injecting model.layers.31.mlp.gate_proj as default
Injecting model.layers.31.mlp.up_proj as default
Injecting model.layers.31.mlp.down_proj as default
Injecting model.layers.31.mlp.act_fn as default
Injecting model.layers.31.input_layernorm as default
Injecting model.layers.31.post_attention_layernorm as default
Injecting model.norm as default
Injecting model.rotary_emb as ktransformers.operators.RoPE . RotaryEmbeddingV2
Injecting lm_head as default
loading token_embd.weight to cpu
loading blk.0.attn_q.weight to cuda:0
loading blk.0.attn_k.weight to cuda:0
loading blk.0.attn_v.weight to cuda:0
loading blk.0.attn_output.weight to cuda:0
loading blk.0.ffn_gate.weight to cuda:0
loading blk.0.ffn_up.weight to cuda:0
loading blk.0.ffn_down.weight to cuda:0
loading blk.0.attn_norm.weight to cuda:0
loading blk.0.ffn_norm.weight to cuda:0
loading blk.1.attn_q.weight to cuda:0
loading blk.1.attn_k.weight to cuda:0
loading blk.1.attn_v.weight to cuda:0
loading blk.1.attn_output.weight to cuda:0
loading blk.1.ffn_gate.weight to cuda:0
loading blk.1.ffn_up.weight to cuda:0
loading blk.1.ffn_down.weight to cuda:0
loading blk.1.attn_norm.weight to cuda:0
loading blk.1.ffn_norm.weight to cuda:0
loading blk.2.attn_q.weight to cuda:0
loading blk.2.attn_k.weight to cuda:0
loading blk.2.attn_v.weight to cuda:0
loading blk.2.attn_output.weight to cuda:0
loading blk.2.ffn_gate.weight to cuda:0
loading blk.2.ffn_up.weight to cuda:0
loading blk.2.ffn_down.weight to cuda:0
loading blk.2.attn_norm.weight to cuda:0
loading blk.2.ffn_norm.weight to cuda:0
loading blk.3.attn_q.weight to cuda:0
loading blk.3.attn_k.weight to cuda:0
loading blk.3.attn_v.weight to cuda:0
loading blk.3.attn_output.weight to cuda:0
loading blk.3.ffn_gate.weight to cuda:0
loading blk.3.ffn_up.weight to cuda:0
loading blk.3.ffn_down.weight to cuda:0
loading blk.3.attn_norm.weight to cuda:0
loading blk.3.ffn_norm.weight to cuda:0
loading blk.4.attn_q.weight to cuda:0
loading blk.4.attn_k.weight to cuda:0
loading blk.4.attn_v.weight to cuda:0
loading blk.4.attn_output.weight to cuda:0
loading blk.4.ffn_gate.weight to cuda:0
loading blk.4.ffn_up.weight to cuda:0
loading blk.4.ffn_down.weight to cuda:0
loading blk.4.attn_norm.weight to cuda:0
loading blk.4.ffn_norm.weight to cuda:0
loading blk.5.attn_q.weight to cuda:0
loading blk.5.attn_k.weight to cuda:0
loading blk.5.attn_v.weight to cuda:0
loading blk.5.attn_output.weight to cuda:0
loading blk.5.ffn_gate.weight to cuda:0
loading blk.5.ffn_up.weight to cuda:0
loading blk.5.ffn_down.weight to cuda:0
loading blk.5.attn_norm.weight to cuda:0
loading blk.5.ffn_norm.weight to cuda:0
loading blk.6.attn_q.weight to cuda:0
loading blk.6.attn_k.weight to cuda:0
loading blk.6.attn_v.weight to cuda:0
loading blk.6.attn_output.weight to cuda:0
loading blk.6.ffn_gate.weight to cuda:0
loading blk.6.ffn_up.weight to cuda:0
loading blk.6.ffn_down.weight to cuda:0
loading blk.6.attn_norm.weight to cuda:0
loading blk.6.ffn_norm.weight to cuda:0
loading blk.7.attn_q.weight to cuda:0
loading blk.7.attn_k.weight to cuda:0
loading blk.7.attn_v.weight to cuda:0
loading blk.7.attn_output.weight to cuda:0
loading blk.7.ffn_gate.weight to cuda:0
loading blk.7.ffn_up.weight to cuda:0
loading blk.7.ffn_down.weight to cuda:0
loading blk.7.attn_norm.weight to cuda:0
loading blk.7.ffn_norm.weight to cuda:0
loading blk.8.attn_q.weight to cuda:0
loading blk.8.attn_k.weight to cuda:0
loading blk.8.attn_v.weight to cuda:0
loading blk.8.attn_output.weight to cuda:0
loading blk.8.ffn_gate.weight to cuda:0
loading blk.8.ffn_up.weight to cuda:0
loading blk.8.ffn_down.weight to cuda:0
loading blk.8.attn_norm.weight to cuda:0
loading blk.8.ffn_norm.weight to cuda:0
loading blk.9.attn_q.weight to cuda:0
loading blk.9.attn_k.weight to cuda:0
loading blk.9.attn_v.weight to cuda:0
loading blk.9.attn_output.weight to cuda:0
loading blk.9.ffn_gate.weight to cuda:0
loading blk.9.ffn_up.weight to cuda:0
loading blk.9.ffn_down.weight to cuda:0
loading blk.9.attn_norm.weight to cuda:0
loading blk.9.ffn_norm.weight to cuda:0
loading blk.10.attn_q.weight to cuda:0
loading blk.10.attn_k.weight to cuda:0
loading blk.10.attn_v.weight to cuda:0
loading blk.10.attn_output.weight to cuda:0
loading blk.10.ffn_gate.weight to cuda:0
loading blk.10.ffn_up.weight to cuda:0
loading blk.10.ffn_down.weight to cuda:0
loading blk.10.attn_norm.weight to cuda:0
loading blk.10.ffn_norm.weight to cuda:0
loading blk.11.attn_q.weight to cuda:0
loading blk.11.attn_k.weight to cuda:0
loading blk.11.attn_v.weight to cuda:0
loading blk.11.attn_output.weight to cuda:0
loading blk.11.ffn_gate.weight to cuda:0
loading blk.11.ffn_up.weight to cuda:0
loading blk.11.ffn_down.weight to cuda:0
loading blk.11.attn_norm.weight to cuda:0
loading blk.11.ffn_norm.weight to cuda:0
loading blk.12.attn_q.weight to cuda:0
loading blk.12.attn_k.weight to cuda:0
loading blk.12.attn_v.weight to cuda:0
loading blk.12.attn_output.weight to cuda:0
loading blk.12.ffn_gate.weight to cuda:0
loading blk.12.ffn_up.weight to cuda:0
loading blk.12.ffn_down.weight to cuda:0
loading blk.12.attn_norm.weight to cuda:0
loading blk.12.ffn_norm.weight to cuda:0
loading blk.13.attn_q.weight to cuda:0
loading blk.13.attn_k.weight to cuda:0
loading blk.13.attn_v.weight to cuda:0
loading blk.13.attn_output.weight to cuda:0
loading blk.13.ffn_gate.weight to cuda:0
loading blk.13.ffn_up.weight to cuda:0
loading blk.13.ffn_down.weight to cuda:0
loading blk.13.attn_norm.weight to cuda:0
loading blk.13.ffn_norm.weight to cuda:0
loading blk.14.attn_q.weight to cuda:0
loading blk.14.attn_k.weight to cuda:0
loading blk.14.attn_v.weight to cuda:0
loading blk.14.attn_output.weight to cuda:0
loading blk.14.ffn_gate.weight to cuda:0
loading blk.14.ffn_up.weight to cuda:0
loading blk.14.ffn_down.weight to cuda:0
loading blk.14.attn_norm.weight to cuda:0
loading blk.14.ffn_norm.weight to cuda:0
loading blk.15.attn_q.weight to cuda:0
loading blk.15.attn_k.weight to cuda:0
loading blk.15.attn_v.weight to cuda:0
loading blk.15.attn_output.weight to cuda:0
loading blk.15.ffn_gate.weight to cuda:0
loading blk.15.ffn_up.weight to cuda:0
loading blk.15.ffn_down.weight to cuda:0
loading blk.15.attn_norm.weight to cuda:0
loading blk.15.ffn_norm.weight to cuda:0
loading blk.16.attn_q.weight to cuda:0
loading blk.16.attn_k.weight to cuda:0
loading blk.16.attn_v.weight to cuda:0
loading blk.16.attn_output.weight to cuda:0
loading blk.16.ffn_gate.weight to cuda:0
loading blk.16.ffn_up.weight to cuda:0
loading blk.16.ffn_down.weight to cuda:0
loading blk.16.attn_norm.weight to cuda:0
loading blk.16.ffn_norm.weight to cuda:0
loading blk.17.attn_q.weight to cuda:0
loading blk.17.attn_k.weight to cuda:0
loading blk.17.attn_v.weight to cuda:0
loading blk.17.attn_output.weight to cuda:0
loading blk.17.ffn_gate.weight to cuda:0
loading blk.17.ffn_up.weight to cuda:0
loading blk.17.ffn_down.weight to cuda:0
loading blk.17.attn_norm.weight to cuda:0
loading blk.17.ffn_norm.weight to cuda:0
loading blk.18.attn_q.weight to cuda:0
loading blk.18.attn_k.weight to cuda:0
loading blk.18.attn_v.weight to cuda:0
loading blk.18.attn_output.weight to cuda:0
loading blk.18.ffn_gate.weight to cuda:0
[H[2J[3Jloading blk.18.ffn_up.weight to cuda:0
loading blk.18.ffn_down.weight to cuda:0
loading blk.18.attn_norm.weight to cuda:0
loading blk.18.ffn_norm.weight to cuda:0
loading blk.19.attn_q.weight to cuda:0
loading blk.19.attn_k.weight to cuda:0
loading blk.19.attn_v.weight to cuda:0
loading blk.19.attn_output.weight to cuda:0
loading blk.19.ffn_gate.weight to cuda:0
loading blk.19.ffn_up.weight to cuda:0
loading blk.19.ffn_down.weight to cuda:0
loading blk.19.attn_norm.weight to cuda:0
loading blk.19.ffn_norm.weight to cuda:0
loading blk.20.attn_q.weight to cuda:0
loading blk.20.attn_k.weight to cuda:0
loading blk.20.attn_v.weight to cuda:0
loading blk.20.attn_output.weight to cuda:0
loading blk.20.ffn_gate.weight to cuda:0
loading blk.20.ffn_up.weight to cuda:0
loading blk.20.ffn_down.weight to cuda:0
loading blk.20.attn_norm.weight to cuda:0
loading blk.20.ffn_norm.weight to cuda:0
loading blk.21.attn_q.weight to cuda:0
loading blk.21.attn_k.weight to cuda:0
loading blk.21.attn_v.weight to cuda:0
loading blk.21.attn_output.weight to cuda:0
loading blk.21.ffn_gate.weight to cuda:0
loading blk.21.ffn_up.weight to cuda:0
loading blk.21.ffn_down.weight to cuda:0
loading blk.21.attn_norm.weight to cuda:0
loading blk.21.ffn_norm.weight to cuda:0
loading blk.22.attn_q.weight to cuda:0
loading blk.22.attn_k.weight to cuda:0
loading blk.22.attn_v.weight to cuda:0
loading blk.22.attn_output.weight to cuda:0
loading blk.22.ffn_gate.weight to cuda:0
loading blk.22.ffn_up.weight to cuda:0
loading blk.22.ffn_down.weight to cuda:0
loading blk.22.attn_norm.weight to cuda:0
loading blk.22.ffn_norm.weight to cuda:0
loading blk.23.attn_q.weight to cuda:0
loading blk.23.attn_k.weight to cuda:0
loading blk.23.attn_v.weight to cuda:0
loading blk.23.attn_output.weight to cuda:0
loading blk.23.ffn_gate.weight to cuda:0
loading blk.23.ffn_up.weight to cuda:0
loading blk.23.ffn_down.weight to cuda:0
loading blk.23.attn_norm.weight to cuda:0
loading blk.23.ffn_norm.weight to cuda:0
loading blk.24.attn_q.weight to cuda:0
loading blk.24.attn_k.weight to cuda:0
loading blk.24.attn_v.weight to cuda:0
loading blk.24.attn_output.weight to cuda:0
loading blk.24.ffn_gate.weight to cuda:0
loading blk.24.ffn_up.weight to cuda:0
loading blk.24.ffn_down.weight to cuda:0
loading blk.24.attn_norm.weight to cuda:0
loading blk.24.ffn_norm.weight to cuda:0
loading blk.25.attn_q.weight to cuda:0
loading blk.25.attn_k.weight to cuda:0
loading blk.25.attn_v.weight to cuda:0
loading blk.25.attn_output.weight to cuda:0
loading blk.25.ffn_gate.weight to cuda:0
loading blk.25.ffn_up.weight to cuda:0
loading blk.25.ffn_down.weight to cuda:0
loading blk.25.attn_norm.weight to cuda:0
loading blk.25.ffn_norm.weight to cuda:0
loading blk.26.attn_q.weight to cuda:0
loading blk.26.attn_k.weight to cuda:0
loading blk.26.attn_v.weight to cuda:0
loading blk.26.attn_output.weight to cuda:0
loading blk.26.ffn_gate.weight to cuda:0
loading blk.26.ffn_up.weight to cuda:0
loading blk.26.ffn_down.weight to cuda:0
loading blk.26.attn_norm.weight to cuda:0
loading blk.26.ffn_norm.weight to cuda:0
loading blk.27.attn_q.weight to cuda:0
loading blk.27.attn_k.weight to cuda:0
loading blk.27.attn_v.weight to cuda:0
loading blk.27.attn_output.weight to cuda:0
loading blk.27.ffn_gate.weight to cuda:0
loading blk.27.ffn_up.weight to cuda:0
loading blk.27.ffn_down.weight to cuda:0
loading blk.27.attn_norm.weight to cuda:0
loading blk.27.ffn_norm.weight to cuda:0
loading blk.28.attn_q.weight to cuda:0
loading blk.28.attn_k.weight to cuda:0
loading blk.28.attn_v.weight to cuda:0
loading blk.28.attn_output.weight to cuda:0
loading blk.28.ffn_gate.weight to cuda:0
loading blk.28.ffn_up.weight to cuda:0
loading blk.28.ffn_down.weight to cuda:0
loading blk.28.attn_norm.weight to cuda:0
loading blk.28.ffn_norm.weight to cuda:0
loading blk.29.attn_q.weight to cuda:0
loading blk.29.attn_k.weight to cuda:0
loading blk.29.attn_v.weight to cuda:0
loading blk.29.attn_output.weight to cuda:0
loading blk.29.ffn_gate.weight to cuda:0
loading blk.29.ffn_up.weight to cuda:0
loading blk.29.ffn_down.weight to cuda:0
loading blk.29.attn_norm.weight to cuda:0
loading blk.29.ffn_norm.weight to cuda:0
loading blk.30.attn_q.weight to cuda:0
loading blk.30.attn_k.weight to cuda:0
loading blk.30.attn_v.weight to cuda:0
loading blk.30.attn_output.weight to cuda:0
loading blk.30.ffn_gate.weight to cuda:0
loading blk.30.ffn_up.weight to cuda:0
loading blk.30.ffn_down.weight to cuda:0
loading blk.30.attn_norm.weight to cuda:0
loading blk.30.ffn_norm.weight to cuda:0
loading blk.31.attn_q.weight to cuda:0
loading blk.31.attn_k.weight to cuda:0
loading blk.31.attn_v.weight to cuda:0
loading blk.31.attn_output.weight to cuda:0
loading blk.31.ffn_gate.weight to cuda:0
loading blk.31.ffn_up.weight to cuda:0
loading blk.31.ffn_down.weight to cuda:0
loading blk.31.attn_norm.weight to cuda:0
loading blk.31.ffn_norm.weight to cuda:0
loading output_norm.weight to cuda:0
loading output.weight to cuda:0
Chat: layer_num: 32, kv_head_num: 8, q_head_num: 32, head_dim: 128, block_len: 128, anchor_num: 1, anchor_type: DYNAMIC, kv_type: GGML_TYPE_F16, retrieval_type: SHARED, layer_step: 1, token_step: 1, layer_offset: 0,max_block_num: 781, max_batch_size: 1, max_thread_num: 48
Traceback (most recent call last):
  File "/home/ziyang/nvme7n1p1_mount/ktransformers/ktransformers/local_chat.py", line 163, in <module>
    fire.Fire(local_chat)
  File "/home/ziyang/micromamba/lib/python3.10/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/ziyang/micromamba/lib/python3.10/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/ziyang/micromamba/lib/python3.10/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/ziyang/nvme7n1p1_mount/ktransformers/ktransformers/local_chat.py", line 121, in local_chat
    content = input("Chat: ")
KeyboardInterrupt
